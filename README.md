**The Effect of Bootstrapping Deep Ensembles
on Explained Predictive Uncertainty**

**Abstract**
This bachelor thesis investigates the impact of bootstrapping on deep ensembles in the
context of explained predictive uncertainty in machine learning models. As the need for not
just accurate but also interpretable and reliable predictions grows, understanding and quantifying
predictive uncertainty becomes essential. Deep ensembles are well-known to deliver
both accurate predictions and reliable uncertainty estimates. However, they do not account
for some sources of uncertainty due to the finite nature of the dataset samples. This work
explores how introducing sampling variability through bootstrapping affects uncertainty estimates
in deep ensembles and especially the explanations of predictive uncertainty. In this,
it focuses on trustworthiness of the estimates and the reliability of uncertainty explanations,
particularly by looking at second-order effects. Through an experimental analysis, this thesis
tests hypotheses regarding the influence of na¨ıve and parametric bootstraps on model diversity,
calibration, and the sensitivity to spurious correlations in experiments on explained predictive
uncertainty. This thesis will show the potential usefulness of bootstrapping deep ensembles
in the context of explaining predictive uncertainty. It will highlight that bootstrapping deep
ensembles increases epistemic uncertainty estimates and improves metrics such as negative
log-likelihood and calibration. Furthermore, this thesis will demonstrate that in certain scenarios,
bootstrapping is able to detect relevant noisy features in second-order explanations
better than non-bootstrapped deep ensembles.
